{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: SVM Spam Classifier\n",
    "\n",
    "### 1- Basic notions about machine learning\n",
    "- Give the steps of the algorithm of best subset selection.\n",
    "- What is the difference between supervised and unsupervised learning?\n",
    "- Describe a real-life situation in which linear regression might be useful (specify thefeatures, design the predictor) and transforming the predictor logistic regression might be also used. Describe that transformation of the predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Programming Section\n",
    "In this project, you are invited to work with spam email dataset from Apache SpamAssas- sin Project. First, you should train an SVM classifier using ”spam train.txt” file in github. This file contains 1899 features per email (line). Each features represent the number of oc- currence of a given word from ”vocab list.txt” file. In fact, raw emails from SpamAssassin project are preprocessed to substitute or remove some expression and punctuation. Then, we select the most occurring words in the resulting emails to build a vocabulary list. In our case, we select all words which occur at least a 100 times in all emails. This results in a list of 1899 words. In practice, a vocabulary list with about 10 000 to 50 000 words is often used.\n",
    "After training the SVM Classifier, you should test it on raw emails. You have in github folder some file containing email samples. You could also use some email from your mail- box. You should preprocess these raw emails to extract features. Thus, you could feed the features vector to your trained classifier that predict if it is spam email or not. Finally, you should evaluate the generalized performance of your spam classifier on the given test set.\n",
    "\n",
    "### Preprocessing Emails\n",
    "In general emails contain different types of entities (e.g. numbers, dollar amount, URLs, or other email addresses). These entities will be different in almost every email. Therefore, one method often employed in processing emails is to normalize these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string httpaddr to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typ- ically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "Below needed preprocessing and normalization steps are enumerated. Besides, some regular expressions and function usefull for these processing are given:\n",
    "\n",
    "- Lower-casing: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "- Stripping HTML: All HTML tags are removed from the emails. Many emails often come with HTML formatting. You should remove all the HTML tags by replacing this regular expression ’<[^<>]+>’ with white space. You could use ”sub()” function from regular expression library in python.\n",
    "- Normalizing URLs: All URLs are normalized by replacing this regular expression ’(http|https)://[^\\s]*’ with the text ’httpaddr’.\n",
    "- Normalizing Email Addresses: All email addresses are normalized by replacing this regular expression ’[^\\s]+@[^\\s]+’ with the text ’emailaddr’.\n",
    "- Normalizing Numbers: All numbers are normalized by replacing this regular expression ’[0-9]+’ with the text ’number’.\n",
    "- Normalizing Dollars: All dollar signs (**$**) are replaced with the text dollar (find the appropriate regular expression to use).\n",
    "\n",
    "- Removal of non-words: Non-words and punctuation should be removed and all white spaces (tabs, newlines, spaces) have to be trimmed to a single space character by replacing the following regular expression with a single white space.\n",
    "      ’[@/#.\\-:\\[\\]&*+=?!(){},\\’\\’\">_<;% \\t\\n\\r]+’\n",
    "Concerning the leading and trailing space you could removed them using ”strip()” function.\n",
    "- Word Stemming: Words are reduced to their stemmed form. For example, discount, discounts, discounted and discounting are all replaced with discount. Sometimes, the Stemmer actually strips off additional characters from the end, so include, includes, included, and including are all replaced with includ. You may use PorterStemmer class from nltk.stem module. For that you need to download ”punkt” package using this command in python: nltk.download(’punkt’)\n",
    "\n",
    "<font color=\"blue\">**Question 1: **</font> Load the dataset from ”spam_train.txt” file in github and explore it. Try to solve the problem of missing value if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of data is: (4000, 1900)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load and extract data\n",
    "data = np.loadtxt(\"spam_train.txt\",dtype=\"object\")\n",
    "print(\"The size of data is:\", data.shape)\n",
    "\n",
    "# Solving problem of missing value, considering that the missing values have type NAN\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna()\n",
    "\n",
    "m = df.shape[0]   # number of emails\n",
    "y = df[df.columns[-1:]].values.flatten().astype(np.float) \n",
    "X = df[df.columns[0:-1]].values.astype(np.float) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2: **</font> Train an SVM classifier using loaded training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create and train SVM classifier\n",
    "lambda_ = 1\n",
    "C = 1/lambda_\n",
    "lin_svm = svm.SVC(kernel=\"linear\")\n",
    "lin_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 3: **</font> Introduce the regularization on your model if it wasnt used and tune its regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of test data is: (1000, 1900)\n",
      "(1000, 1899)\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 0.01  is:  97.8 %\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 0.03  is:  97.8 %\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 0.1  is:  97.8 %\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 0.3  is:  97.8 %\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 1  is:  97.8 %\n",
      "The 5-fold cross validation mean accuracy for 'Λ= 100  is:  97.8 %\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "# load and extract data test\n",
    "test = np.loadtxt(\"spam_test.txt\")\n",
    "print(\"The size of test data is:\", test.shape)\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(5190969)\n",
    "test=np.random.permutation(test)\n",
    "\n",
    "# Solving problem of missing value, considering that the missing values have type NAN\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df.dropna()\n",
    "\n",
    "#extract data\n",
    "m = test_df.shape[0] # number of samples\n",
    "y_test = test_df[test_df.columns[-1:]].values.flatten().astype(np.float)  \n",
    "X_test = test_df[test_df.columns[0:-1]].values.astype(np.float) \n",
    "\n",
    "\n",
    "C = 1  # regularization parameter\n",
    "K = 5  # number of cross validation folds\n",
    "fold_size = int(floor(m/K))  #size of fold\n",
    "lambda_list = [0.01, 0.03, 0.1,0.3, 1,100]\n",
    "print(X_test.shape)\n",
    "\n",
    "# loop over different sigma values\n",
    "for lambda_ in lambda_list:\n",
    "    C = 1/lambda_\n",
    "    val_accuracy=np.zeros((K,))\n",
    "    # loop over different folds and calculate the mean accuracy\n",
    "    for k in range(K):\n",
    "        X_val = X_test[k*fold_size:(k+1)*fold_size]  \n",
    "        y_val = y_test[k*fold_size:(k+1)*fold_size]\n",
    "        gauss_svm = svm.SVC(C=C,kernel='linear')\n",
    "        gauss_svm.fit(X, y)\n",
    "        y_val_pred = lin_svm.predict(X_val).astype(float)\n",
    "        val_accuracy[k]=(y_val==y_val_pred).sum()/fold_size*100\n",
    "    \n",
    "    mean_val_accuracy = val_accuracy.mean()\n",
    "    print(\"The {}-fold cross validation mean accuracy for '\\u039B=\".format(K), lambda_,\" is: \",mean_val_accuracy,\"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 4: **</font> Implement the preprocessing steps described above. Try to use given regular expressions and functions that may help. Then, you should split the email with white space character in order to get a list of words. Thus, you could apply stemming word by word. However, you need to remove any non-alphanumeric character from each word by replacing this regular expression ’[^a-zA-Z0-9]’ with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucascoiado/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def preprocessing(email):\n",
    "    email=email.lower()  # Lower-casing\n",
    "    email= re.sub(r'<[^<>]+>',r' ',email) # Stripping HTML\n",
    "    email= re.sub(r'(http|https)://[^\\s]*',r'httpaddr',email) # Normalizing URLs\n",
    "    email= re.sub(r'[^\\s]+@[^\\s]+',r'emailaddr',email) # Normalizing Email Addresses\n",
    "    email= re.sub(r'[0-9]+',r'number',email) # Normalizing Numbers\n",
    "    email= re.sub(r'\\$',r'dollar',email) # Normalizing Dollars\n",
    "    email= re.sub(r'[@/#.\\-:\\[\\]&*+=?!(){},\\’\\’\">_<;% \\t\\n\\r]+',r' ',email) # Removal of non-words\n",
    "    email = email.strip()  \n",
    "    words = email.split()\n",
    "    email_final = []\n",
    "\n",
    "    for w in words:\n",
    "        w= re.sub(r'[^a-zA-Z0-9]',r'',w)\n",
    "        email_final.append(ps.stem(w)) # Word Stemming\n",
    "    \n",
    "    return email_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 5: **</font> Load the vocabulary list (”vocab list.txt” file in github) and extract features from the processed email. For that you should count the occurrence in the email of each word in the vocabulary list. Then, put this count in a vector of 1899 elements according to the order of word in the vocabulary list. For example, if the 3rd element of the resulting features vector is 5 this means that the 3rd word in the vocabulary list was encountered 5 times in the processed email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def occurrences(email):\n",
    "    email = preprocessing(email)\n",
    "    # load and extract vocabulary \n",
    "    vocabul = np.loadtxt(\"vocab_list.txt\",dtype='str')[:,1,np.newaxis]\n",
    "    occurrences = np.zeros((1,1899))\n",
    "\n",
    "    # Calculating the occurrences\n",
    "    for index,w in enumerate(vocabul):\n",
    "        occurrences[:,index] = email.count(w)\n",
    "    return occurrences\n",
    "\n",
    "with open('emailSample1.txt', 'r') as myfile:\n",
    "    email1=myfile.read().replace('\\n', '')\n",
    "    occu1 = occurrences(email1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 6: **</font> Estimate the generalized performance of this model using adequate metrics.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sklearn confusion matrix:\n",
      " [[679  13]\n",
      " [  9 299]]\n",
      "The sklearn precision: 0.958333333333\n",
      "The sklearn recall: 0.970779220779\n",
      "The sklearn F1_score: 0.964516129032\n",
      "The sklearn classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.98      0.98       692\n",
      "        1.0       0.96      0.97      0.96       308\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1000\n",
      "\n",
      "Prediction of Samples emails: [ 0.  0.  1.  1.]\n",
      "Prediction of our own emails: [ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "from  sklearn import metrics\n",
    "# SVM classifier\n",
    "gauss_svm = svm.SVC(C=C,kernel='linear')\n",
    "gauss_svm.fit(X, y)\n",
    "y_pred =  lin_svm.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"The sklearn confusion matrix:\\n\",metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"The sklearn precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"The sklearn recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"The sklearn F1_score:\",metrics.f1_score(y_test, y_pred))\n",
    "print(\"The sklearn classification report:\\n\",metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Load the given emails\n",
    "with open('emailSample1.txt', 'r') as myfile:\n",
    "    email1=myfile.read().replace('\\n', '')\n",
    "    occu1 = occurrences(email1)\n",
    "    \n",
    "with open('emailSample2.txt', 'r') as myfile:\n",
    "    email2=myfile.read().replace('\\n', '')\n",
    "    occu2 = occurrences(email2)\n",
    "\n",
    "with open('spamSample1.txt', 'r') as myfile:\n",
    "    email3=myfile.read().replace('\\n', '')\n",
    "    occu3 = occurrences(email3)\n",
    "    \n",
    "with open('spamSample2.txt', 'r') as myfile:\n",
    "    email4=myfile.read().replace('\\n', '')\n",
    "    occu4 = occurrences(email4)\n",
    "\n",
    "# Test with given emails\n",
    "occu =np.concatenate((occu1,occu2,occu3,occu4),axis=0)\n",
    "y_pred =  lin_svm.predict(occu) \n",
    "\n",
    "print(\"Prediction of Samples emails:\", y_pred)\n",
    "\n",
    "# Testing with own emails\n",
    "with open('Spam.txt', 'r') as myfile:\n",
    "    spam=myfile.read().replace('\\n', '')\n",
    "    occu5 = occurrences(spam)\n",
    "    \n",
    "    y_pred =  lin_svm.predict(occu5) \n",
    "    \n",
    "with open('Email.txt', 'r') as myfile:\n",
    "    email=myfile.read().replace('\\n', '')\n",
    "    occu6 = occurrences(email)\n",
    "\n",
    "occu =np.concatenate((occu5,occu6),axis=0)\n",
    "y_pred =  lin_svm.predict(occu)\n",
    "print(\"Prediction of our own emails:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
