{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: SVM Spam Classifier\n",
    "\n",
    "### 1- Basic notions about machine learning\n",
    "- Give the steps of the algorithm of best subset selection.\n",
    "- What is the difference between supervised and unsupervised learning?\n",
    "- Describe a real-life situation in which linear regression might be useful (specify thefeatures, design the predictor) and transforming the predictor logistic regression might be also used. Describe that transformation of the predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Programming Section\n",
    "In this project, you are invited to work with spam email dataset from Apache SpamAssas- sin Project. First, you should train an SVM classifier using ”spam train.txt” file in github. This file contains 1899 features per email (line). Each features represent the number of oc- currence of a given word from ”vocab list.txt” file. In fact, raw emails from SpamAssassin project are preprocessed to substitute or remove some expression and punctuation. Then, we select the most occurring words in the resulting emails to build a vocabulary list. In our case, we select all words which occur at least a 100 times in all emails. This results in a list of 1899 words. In practice, a vocabulary list with about 10 000 to 50 000 words is often used.\n",
    "After training the SVM Classifier, you should test it on raw emails. You have in github folder some file containing email samples. You could also use some email from your mail- box. You should preprocess these raw emails to extract features. Thus, you could feed the features vector to your trained classifier that predict if it is spam email or not. Finally, you should evaluate the generalized performance of your spam classifier on the given test set.\n",
    "\n",
    "### Preprocessing Emails\n",
    "In general emails contain different types of entities (e.g. numbers, dollar amount, URLs, or other email addresses). These entities will be different in almost every email. Therefore, one method often employed in processing emails is to normalize these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string httpaddr to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typ- ically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "Below needed preprocessing and normalization steps are enumerated. Besides, some regular expressions and function usefull for these processing are given:\n",
    "\n",
    "- Lower-casing: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "- Stripping HTML: All HTML tags are removed from the emails. Many emails often come with HTML formatting. You should remove all the HTML tags by replacing this regular expression ’<[^<>]+>’ with white space. You could use ”sub()” function from regular expression library in python.\n",
    "- Normalizing URLs: All URLs are normalized by replacing this regular expression ’(http|https)://[^\\s]*’ with the text ’httpaddr’.\n",
    "- Normalizing Email Addresses: All email addresses are normalized by replacing this regular expression ’[^\\s]+@[^\\s]+’ with the text ’emailaddr’.\n",
    "- Normalizing Numbers: All numbers are normalized by replacing this regular expression ’[0-9]+’ with the text ’number’.\n",
    "- Normalizing Dollars: All dollar signs (**$**) are replaced with the text dollar (find the appropriate regular expression to use).\n",
    "\n",
    "- Removal of non-words: Non-words and punctuation should be removed and all white spaces (tabs, newlines, spaces) have to be trimmed to a single space character by replacing the following regular expression with a single white space.\n",
    "      ’[@/#.\\-:\\[\\]&*+=?!(){},\\’\\’\">_<;% \\t\\n\\r]+’\n",
    "Concerning the leading and trailing space you could removed them using ”strip()” function.\n",
    "- Word Stemming: Words are reduced to their stemmed form. For example, discount, discounts, discounted and discounting are all replaced with discount. Sometimes, the Stemmer actually strips off additional characters from the end, so include, includes, included, and including are all replaced with includ. You may use PorterStemmer class from nltk.stem module. For that you need to download ”punkt” package using this command in python: nltk.download(’punkt’)\n",
    "\n",
    "<font color=\"blue\">**Question 1: **</font> Load the dataset from ”spam_train.txt” file in github and explore it. Try to solve the problem of missing value if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of data is: (4000, 1900)\n",
      "     0    1    2    3    4    5    6    7    8    9    ...  1890 1891 1892  \\\n",
      "0       0    0    0    0    0    0    0    0    0    1 ...     0    0    0   \n",
      "1       0    0    0    0    1    1    0    0    0    1 ...     0    1    1   \n",
      "2       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "3       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "4       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "5       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "6       0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "7       0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "8       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "9       0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "10      0    0    0    0    1    0    1    0    0    0 ...     0    0    1   \n",
      "11      0    0    0    1    1    0    0    0    0    0 ...     0    0    1   \n",
      "12      0    0    0    0    1    0    0    0    0    0 ...     0    0    0   \n",
      "13      0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "14      0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "15      0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "16      0    0    0    0    1    0    0    0    0    0 ...     1    0    1   \n",
      "17      0    0    0    0    1    0    0    1    0    0 ...     1    0    1   \n",
      "18      0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "19      0    0    0    0    0    0    0    0    0    1 ...     0    0    1   \n",
      "20      0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "21      1    1    0    0    0    0    0    0    1    0 ...     0    0    1   \n",
      "22      0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "23      0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "24      0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "25      0    0    0    0    0    0    1    0    0    0 ...     0    0    1   \n",
      "26      0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "27      0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "28      0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "29      0    0    0    0    0    1    0    0    0    0 ...     0    0    0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...  ...  ...   \n",
      "3970    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3971    0    0    1    1    1    0    0    0    0    0 ...     0    1    1   \n",
      "3972    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3973    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3974    0    0    0    1    0    0    0    0    0    0 ...     0    0    0   \n",
      "3975    0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "3976    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3977    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3978    1    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "3979    0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "3980    0    0    1    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3981    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3982    0    0    0    1    1    0    0    0    0    0 ...     0    0    1   \n",
      "3983    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3984    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3985    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3986    0    0    0    1    0    0    0    0    0    0 ...     0    0    1   \n",
      "3987    0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "3988    0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "3989    0    0    0    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "3990    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3991    0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "3992    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3993    0    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
      "3994    0    0    0    0    0    1    0    0    0    0 ...     0    0    1   \n",
      "3995    0    0    0    0    1    0    0    0    0    0 ...     0    0    0   \n",
      "3996    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3997    0    0    0    0    0    0    0    0    0    0 ...     0    0    1   \n",
      "3998    0    0    1    0    1    0    0    0    0    0 ...     0    0    1   \n",
      "3999    0    0    0    0    0    1    0    0    0    0 ...     0    0    0   \n",
      "\n",
      "     1893 1894 1895 1896 1897 1898 1899  \n",
      "0       0    1    0    0    0    0    1  \n",
      "1       0    1    0    0    0    0    1  \n",
      "2       0    0    0    0    0    0    0  \n",
      "3       0    0    0    0    0    0    0  \n",
      "4       0    0    0    0    0    0    0  \n",
      "5       0    0    0    0    0    0    0  \n",
      "6       0    1    0    0    0    0    1  \n",
      "7       0    1    0    0    0    0    0  \n",
      "8       0    0    0    0    0    0    0  \n",
      "9       0    0    0    0    0    0    0  \n",
      "10      0    1    0    0    0    0    1  \n",
      "11      0    1    0    0    0    0    0  \n",
      "12      0    0    0    0    0    0    0  \n",
      "13      0    0    0    0    0    0    0  \n",
      "14      0    0    0    0    0    0    1  \n",
      "15      0    0    0    0    0    0    0  \n",
      "16      0    0    0    0    0    0    0  \n",
      "17      0    0    0    0    0    0    0  \n",
      "18      0    1    0    0    0    0    1  \n",
      "19      0    1    0    0    0    0    1  \n",
      "20      0    0    0    0    0    0    0  \n",
      "21      0    0    0    0    0    0    1  \n",
      "22      0    0    0    0    0    0    0  \n",
      "23      0    0    0    0    0    0    1  \n",
      "24      0    1    0    0    0    0    1  \n",
      "25      0    1    0    0    0    1    1  \n",
      "26      0    0    0    0    0    0    1  \n",
      "27      0    0    0    0    0    0    0  \n",
      "28      0    0    0    0    0    0    0  \n",
      "29      0    1    0    0    0    0    0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "3970    0    1    0    0    0    0    0  \n",
      "3971    0    0    0    0    1    0    0  \n",
      "3972    0    0    0    0    0    0    0  \n",
      "3973    0    0    0    0    0    0    0  \n",
      "3974    0    0    0    0    0    0    0  \n",
      "3975    0    0    0    0    0    0    0  \n",
      "3976    0    1    0    0    0    0    1  \n",
      "3977    0    0    0    0    0    0    0  \n",
      "3978    0    1    0    0    0    0    1  \n",
      "3979    0    1    0    0    0    0    0  \n",
      "3980    0    1    0    0    0    0    1  \n",
      "3981    0    1    0    0    0    0    0  \n",
      "3982    0    1    0    0    0    0    0  \n",
      "3983    0    1    0    0    0    0    0  \n",
      "3984    0    0    0    0    0    0    0  \n",
      "3985    0    0    0    0    0    0    0  \n",
      "3986    0    1    0    0    0    0    0  \n",
      "3987    0    1    0    0    0    0    0  \n",
      "3988    0    1    0    0    0    0    0  \n",
      "3989    0    0    0    0    0    0    0  \n",
      "3990    0    1    0    0    0    0    1  \n",
      "3991    0    0    0    0    0    0    0  \n",
      "3992    0    0    0    0    0    0    0  \n",
      "3993    0    1    0    0    0    0    0  \n",
      "3994    0    0    0    0    0    0    1  \n",
      "3995    0    0    0    0    0    0    0  \n",
      "3996    0    0    0    0    0    0    0  \n",
      "3997    0    1    0    0    0    0    1  \n",
      "3998    0    1    1    0    0    0    0  \n",
      "3999    0    0    0    0    0    0    0  \n",
      "\n",
      "[4000 rows x 1900 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-be040494417e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# number of emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load and extract data\n",
    "data = np.loadtxt(\"spam_train.txt\",dtype=object)\n",
    "print(\"The size of data is:\", data.shape)\n",
    "\n",
    "# Skip rows, where any value is missing\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna()\n",
    "\n",
    "print(df)\n",
    "\n",
    "m = df.shape[0]   # number of emails\n",
    "y = df[:, 0] \n",
    "X = df[:, 1:-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">**Question 2: **</font> Train an SVM classifier using loaded training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create and train SVM classifier\n",
    "lambda_ = 1\n",
    "C = 1/lambda_\n",
    "lin_svm = svm.SVC(C=C, kernel=\"linear\")\n",
    "lin_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 3: **</font> Introduce the regularization on your model if it wasnt used and tune its regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 4: **</font> Implement the preprocessing steps described above. Try to use given regular expressions and functions that may help. Then, you should split the email with white space character in order to get a list of words. Thus, you could apply stemming word by word. However, you need to remove any non-alphanumeric character from each word by replacing this regular expression ’[^a-zA-Z0-9]’ with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lucascoiado/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a', 'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani', 'visitor', 'your', 'expect', 'thi', 'can', 'be', 'anywher', 'from', 'less', 'than', 'number', 'buck', 'a', 'month', 'to', 'a', 'coupl', 'of', 'dollarnumb', 'you', 'should', 'checkout', 'httpaddr', 'or', 'perhap', 'amazon', 'ecnumb', 'if', 'your', 'run', 'someth', 'big', 'to', 'unsubscrib', 'yourself', 'from', 'thi', 'mail', 'list', 'send', 'an', 'email', 'emailaddr']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "nltk.download('punkt')\n",
    "with open('emailSample1.txt', 'r') as myfile:\n",
    "    email=myfile.read().replace('\\n', '')\n",
    "email=email.lower()\n",
    "email= re.sub(r'<[^<>]+>',r' ',email)\n",
    "email= re.sub(r'(http|https)://[^\\s]*',r'httpaddr',email)\n",
    "email= re.sub(r'[^\\s]+@[^\\s]+',r'emailaddr',email)\n",
    "email= re.sub(r'[0-9]+',r'number',email)\n",
    "email= re.sub(r'\\$',r'dollar',email)\n",
    "email= re.sub(r'[@/#.\\-:\\[\\]&*+=?!(){},\\’\\’\">_<;% \\t\\n\\r]+',r' ',email)\n",
    "email = email.strip()\n",
    "\n",
    "words = email.split()\n",
    "email_final = []\n",
    "\n",
    "for w in words:\n",
    "    w= re.sub(r'[^a-zA-Z0-9]',r'',w)\n",
    "    email_final.append(ps.stem(w))\n",
    "    \n",
    "\n",
    "print(email_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 5: **</font> Load the vocabulary list (”vocab list.txt” file in github) and extract features from the processed email. For that you should count the occurrence in the email of each word in the vocabulary list. Then, put this count in a vector of 1899 elements according to the order of word in the vocabulary list. For example, if the 3rd element of the resulting features vector is 5 this means that the 3rd word in the vocabulary list was encountered 5 times in the processed email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1899, 1)\n"
     ]
    }
   ],
   "source": [
    "# load and extract data\n",
    "vocabul = np.loadtxt(\"vocab_list.txt\",dtype='str')[:,1,np.newaxis]\n",
    "print(vocabul.shape)\n",
    "occurrences = np.zeros((1,1900))\n",
    "\n",
    "for index,w in enumerate(vocabul):\n",
    "    occurrences[:,index+1] = email_final.count(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"blue\">**Question 6: **</font> Estimate the generalized performance of this model using adequate metrics.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
